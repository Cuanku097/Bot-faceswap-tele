{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cuanku097/Bot-faceswap-tele/blob/master/Flux_1_gguf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2NJh1eQX6YP",
        "outputId": "76d385b3-8267-4195-a4ee-a64efae095b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "otSpwxfHK770",
        "outputId": "7c341483-4035-416b-9eac-6f6c1238a63c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:4 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\r                                                                               \rHit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "#Update\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y2TMCXgVLDtw",
        "outputId": "07080f3e-de64-4dc3-9ddc-4ea0de804a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 108 not upgraded.\n",
            "Need to get 1,513 kB of archives.\n",
            "After this operation, 5,441 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1,086 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Fetched 1,513 kB in 1s (1,261 kB/s)\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y wget aria2 libgl1-mesa-glx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C2dBus7WLP6v",
        "outputId": "3a770b6e-685e-4ea5-b06e-e878f6e9fac6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ComfyUI'...\n",
            "remote: Enumerating objects: 30231, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 30231 (delta 9), reused 2 (delta 2), pack-reused 30218 (from 3)\u001b[K\n",
            "Receiving objects: 100% (30231/30231), 70.35 MiB | 19.45 MiB/s, done.\n",
            "Resolving deltas: 100% (20520/20520), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/comfyanonymous/ComfyUI.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "collapsed": true,
        "id": "gihFr__cLnNb",
        "outputId": "3c7c5202-edba-426d-b74f-26de2fb4213c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3053552154.py, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3053552154.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install -r requirements.txt\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "%cd ComfyUI\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ufP19iAnL4hY",
        "outputId": "69e0b484-da67-473c-eef8-e4d834066020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinggy in /usr/local/lib/python3.12/dist-packages (0.0.20)\n"
          ]
        }
      ],
      "source": [
        "#Install Pinggy Tunel\n",
        "!pip install pinggy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVi_yh2vV9my",
        "outputId": "12e6e049-9956-492b-b3a8-1e36b940c584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tunnel1 started URLs: ['http://myrhi-34-73-196-116.a.free.pinggy.link', 'https://myrhi-34-73-196-116.a.free.pinggy.link']\n"
          ]
        }
      ],
      "source": [
        "import pinggy\n",
        "\n",
        "tunnel1 = pinggy.start_tunnel(forwardto=\"localhost:8188\",)\n",
        "\n",
        "print(f\"Tunnel1 started URLs: {tunnel1.urls}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "--cTz1UpN_z5",
        "outputId": "dfa8d49a-15bc-4377-e852-b730e56aa464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\r\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "c679be|\u001b[1;32mOK\u001b[0m  |   230MiB/s|/content/ComfyUI/models/vae/flux_vae.safetensors\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n"
          ]
        }
      ],
      "source": [
        "# --- DOWNLOAD FLUX SCHNELL & COMPONENT ---\n",
        "\n",
        "# 1. Download Model Flux.1 Schnell (FP8 agar ringan di T4)\n",
        "# Letakkan di: /content/ComfyUI/models/checkpoints/\n",
        "#aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors\" -d /content/ComfyUI/models/checkpoints -o flux1-schnell-fp8.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q4_0.gguf\" -d /content/ComfyUI/models/unet -o flux1-dev-Q4_0.gguf\n",
        "# 2. Download VAE (Wajib untuk Flux)\n",
        "# Letakkan di: /content/ComfyUI/models/vae/\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://huggingface.co/StableDiffusionVN/Flux/resolve/main/Vae/flux_vae.safetensors\" -d /content/ComfyUI/models/vae -o flux_vae.safetensors\n",
        "\n",
        "# 3. Download CLIP & T5 (Text Encoders)\n",
        "# Letakkan di: /content/ComfyUI/models/clip/\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors\" -d /content/ComfyUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M \"https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors\" -d /content/ComfyUI/models/clip -o clip_l.safetensors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone https://github.com/city96/ComfyUI-GGUF\n",
        "!pip install -r ComfyUI-GGUF/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3beh1-Z0BWWd",
        "outputId": "d8f68858-121c-41d9-d9be-2c65a0bf15d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ComfyUI/custom_nodes\n",
            "Cloning into 'ComfyUI-GGUF'...\n",
            "remote: Enumerating objects: 814, done.\u001b[K\n",
            "remote: Counting objects: 100% (508/508), done.\u001b[K\n",
            "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
            "remote: Total 814 (delta 458), reused 312 (delta 312), pack-reused 306 (from 2)\u001b[K\n",
            "Receiving objects: 100% (814/814), 189.66 KiB | 7.59 MiB/s, done.\n",
            "Resolving deltas: 100% (543/543), done.\n",
            "Collecting gguf>=0.13.0 (from -r ComfyUI-GGUF/requirements.txt (line 2))\n",
            "  Downloading gguf-0.17.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r ComfyUI-GGUF/requirements.txt (line 4)) (0.2.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r ComfyUI-GGUF/requirements.txt (line 5)) (5.29.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from gguf>=0.13.0->-r ComfyUI-GGUF/requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from gguf>=0.13.0->-r ComfyUI-GGUF/requirements.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from gguf>=0.13.0->-r ComfyUI-GGUF/requirements.txt (line 2)) (4.67.1)\n",
            "Downloading gguf-0.17.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gguf\n",
            "Successfully installed gguf-0.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ComfyUI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me2Fv25_BzQu",
        "outputId": "e765b2ff-d969-47d7-be17-36a6d0823053"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ComfyUI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8H5Elu4VMGAw",
        "outputId": "6982f82f-6578-4ee9-b0e6-80418e62648d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint files will always be loaded safely.\n",
            "Total VRAM 15095 MB, total RAM 12976 MB\n",
            "pytorch version: 2.9.0+cu126\n",
            "Enabled fp16 accumulation.\n",
            "Set vram state to: LOW_VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "Using async weight offloading with 2 streams\n",
            "Enabled pinned memory 12326.0\n",
            "working around nvidia conv3d memory bug.\n",
            "WARNING: You need pytorch with cu130 or higher to use optimized CUDA operations.\n",
            "Found comfy_kitchen backend eager: {'available': True, 'disabled': False, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8', 'scaled_mm_nvfp4']}\n",
            "Found comfy_kitchen backend cuda: {'available': True, 'disabled': True, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8', 'scaled_mm_nvfp4']}\n",
            "Found comfy_kitchen backend triton: {'available': True, 'disabled': True, 'unavailable_reason': None, 'capabilities': ['apply_rope', 'apply_rope1', 'dequantize_nvfp4', 'dequantize_per_tensor_fp8', 'quantize_nvfp4', 'quantize_per_tensor_fp8']}\n",
            "Using pytorch attention\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "ComfyUI version: 0.11.0\n",
            "ComfyUI frontend version: 1.37.11\n",
            "[Prompt Server] web root: /usr/local/lib/python3.12/dist-packages/comfyui_frontend_package/static\n",
            "ComfyUI-GGUF: Allowing full torch compile\n",
            "\n",
            "Import times for custom nodes:\n",
            "   0.0 seconds: /content/ComfyUI/custom_nodes/websocket_image_save.py\n",
            "   0.0 seconds: /content/ComfyUI/custom_nodes/ComfyUI-GGUF\n",
            "\n",
            "Context impl SQLiteImpl.\n",
            "Will assume non-transactional DDL.\n",
            "Assets scan(roots=['models']) completed in 0.043s (created=1, skipped_existing=15, total_seen=16)\n",
            "Starting server\n",
            "\n",
            "To see the GUI go to: http://0.0.0.0:8188\n",
            "got prompt\n",
            "Failed to validate prompt for output 7:\n",
            "* KSampler 5:\n",
            "  - Required input is missing: model\n",
            "Output will be ignored\n",
            "Failed to validate prompt for output 12:\n",
            "Output will be ignored\n",
            "invalid prompt: {'type': 'prompt_outputs_failed_validation', 'message': 'Prompt outputs failed validation', 'details': '', 'extra_info': {}}\n",
            "got prompt\n",
            "Using pytorch attention in VAE\n",
            "Using pytorch attention in VAE\n",
            "VAE load device: cuda:0, offload device: cpu, dtype: torch.float32\n",
            "Requested to load AutoencodingEngine\n",
            "loaded completely; 10027.95 MB usable, 319.75 MB loaded, full load: True\n",
            "clip missing: ['text_projection.weight']\n",
            "Requested to load FluxClipModel_\n",
            "loaded completely;  4777.54 MB loaded, full load: True\n",
            "CLIP/text encoder model load device: cpu, offload device: cpu, current: cpu, dtype: torch.float16\n",
            "gguf qtypes: F16 (476), Q4_0 (304)\n",
            "model weight dtype torch.float16, manual cast: None\n",
            "model_type FLUX\n",
            "Requested to load Flux\n",
            "loaded completely; 13430.78 MB usable, 6602.51 MB loaded, full load: True\n",
            "100% 20/20 [01:10<00:00,  3.51s/it]\n",
            "Prompt executed in 308.32 seconds\n",
            "got prompt\n",
            " 10% 2/20 [00:06<01:02,  3.47s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "!python main.py --listen 0.0.0.0 --lowvram --fast --preview-method auto"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}